# sparsefinder
Code for "Predicting Attention Sparsity in Transformers"


## Pipeline:

1. Train a Transformer-based model for a given task (or, more generally, a model with a small context size) using a sparse activation function for computing attention. 

For example, for MLM, we go to [sparsefinder-mlm repo]() to finetune a RoBERTa model with entmax attention and save the model:
```bash
CUDA_VISIBLE_DEVICES=0 python3 scripts/pretrain_entmax_roberta_512.py \
    --wikipath /path/to/wikidata/ \
    --outdir model_dir
````

Where the new model will be saved in `model_dir`.


2. Then, we extract the Q and K generated by the model for a subset of the data. For example, for extracting attention matrices of RoBERTa from 50 examples:
```bash
CUDA_VISIBLE_DEVICES=0 python3 scripts/extract_entmax_attn.py \
    --wikipath /path/to/wikidata/ \
    --model_path model_dir \
    --out qk-mlm \
    --nsamples 50 \
    --batchsize 1
```

This will create a file called `qk-mlm_enc-attn.pt` containing the following info:
```
{
    'length_src': list of ints, 
    'q': list of query tensors of shape (num_layers, num_heads, length, head_size), 
    'k': list of key tensors of shape (num_layers, num_heads, length, head_size),
}
```


3. Train a projection model. This is the core of the Sparsefinder: projections to a smaller space, which is quantized and then queries and keys falling in the same quantization interval are mapped to the same bucket. 

The model is trained with `quantization.py`. It accepts a myriad of hyperparameters; Use `--help` to see all of them. The experiments however have shown minimal impact of most of them, as well as of introducing a hidden layer.

NB: the code in `quantization.py` may mention "hashing rounds" and "number of projections" interchangeably; the terminology here is not set on stone.

```bash
CUDA_VISIBLE_DEVICES=0 python3 quantization.py \
--data /path/to/qk-mlm_enc-attn.pt \
--rounds 4 \
--share-projectors \
--rescale-queries \
--batch-size 32 \
--lr 0.001 \
--epochs 3 \
--eval-steps 5 \
--add-cls-mask \
--window-size 3 \
--grouping kmeans \
--top_clusters 1 \
--bucket_size 16 \
--threshold 1.5 \
--block-size 1 \
--save projections-blocks-shared-4r-bs1-linear-3ep.pickle
```

The projections will be saved in `projections-blocks-shared-4r-bs1-linear-3ep.pickle`. 

Check `quantization_multiple_runs_lm.py` for more info.

In case we use kmeans clustering, we also need to convert the centroids to torch tensors (using the same args as above):
```bash

python3 convert_kmeans_to_torch_centroids.py \
    --data /path/to/qk-mlm_enc-attn.pt \
    --rounds 4 \
    --bucket_size 16 \
    --num-layers 12 \
    --share-projectors \
    --cluster-rounds 1
```

The centroids will be saved in a dir called `centroids/` with a `.pickle` extension (even though the object is a list of torch tensors).


4. Finally, we go back to [sparsefinder-mlm repo]() and evaluate our trained transformer with the learned sparsefinder's projections:

```bash
CUDA_VISIBLE_DEVICES=0 python3 scripts/pretrain_entmax_roberta_512_marcos.py \
    --pretrained-path /path/to/model_dir \
    --wikipath /path/to/wikidata/ \
    --eval-batch-size 8 \
    --attn_variant sparsefinder_kmeans \
    --projections_path /path/to/projections-blocks-shared-4r-bs1-linear-3ep.pickle \
    --centroids_path centroids/kqs_enc-attn.pt_4r_16s_1n_shared.pickle \
    --window_size 11 \
    --cls_size 2 \
    --top_clusters 2 \
    --bucket_size 16 \
    --finetune_part "all_but_projections" \
    --output_dir "outputs/mlm_sparsefinder_kmeans_16s_11w/"
```

